@ARTICLE{Ho2019-kp,
  title = {Moving beyond P values: data analysis with estimation graphics},
  author = {Ho, Joses and Tumkaya, Tayfun and Aryal, Sameer and Choi, Hyungwon
  and Claridge-Chang, Adam},
  journaltitle = {Nature methods},
  publisher = {Springer Science and Business Media LLC},
  volume = {16},
  pages = {565-566},
  date = {2019-07-19},
  doi = {10.1038/s41592-019-0470-3},
  url = {http://dx.doi.org/10.1038/s41592-019-0470-3},
  file = {ho-2019-moving-beyond-p-values-data-analysis-with-estimation-graphics.pdf}
}

@ARTICLE{Sanchez-Meca2003-tz,
  title = {Effect-size indices for dichotomized outcomes in meta-analysis},
  author = {Sánchez-Meca, Julio and Marín-Martínez, Fulgencio and
  Chacón-Moscoso, Salvador},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {8},
  pages = {448-467},
  date = {2003-12},
  doi = {10.1037/1082-989X.8.4.448},
  abstract = {It is very common to find meta-analyses in which some of the
  studies compare 2 groups on continuous dependent variables and others compare
  groups on dichotomized variables. Integrating all of them in a meta-analysis
  requires an effect-size index in the same metric that can be applied to both
  types of outcomes. In this article, the performance in terms of bias and
  sampling variance of 7 different effect-size indices for estimating the
  population standardized mean difference from a 2 x 2 table is examined by
  Monte Carlo simulation, assuming normal and nonnormal distributions. The
  results show good performance for 2 indices, one based on the probit
  transformation and the other based on the logistic distribution.},
  url = {http://dx.doi.org/10.1037/1082-989X.8.4.448},
  file = {sánchez-meca-2003-effect-size-indices-for-dichotomized-outcomes-in-meta-analysis.pdf}
}

@ARTICLE{Patil2019-ms,
  title = {A visual tool for defining reproducibility and replicability},
  author = {Patil, Prasad and Peng, R and Leek, J},
  journaltitle = {Nature human behaviour},
  publisher = {nature.com},
  volume = {3},
  pages = {650-652},
  date = {2019-06-17},
  doi = {10.1038/s41562-019-0629-z},
  url = {https://idp.nature.com/authorize/casa?redirect_uri=https://www.nature.com/articles/s41562-019-0629-z&casa_token=NIJKFWKowh0AAAAA:MKohWDM9m8UoR4HltfGOBcJ33XBVibdJCayJb-Mh4eHEx_WPzpqVN8xM6FMmiZ2cV30ylSyMrpLcEBFKEw},
  file = {patil-2019-a-visual-tool-for-defining-reproducibility-and-replicability.pdf}
}

@ARTICLE{Miller2024-qi,
  title = {How many participants? How many trials? Maximizing the power of
  reaction time studies},
  author = {Miller, Jeff},
  journaltitle = {Behavior research methods},
  publisher = {Springer Science and Business Media LLC},
  volume = {56},
  pages = {2398-2421},
  date = {2024-03},
  doi = {10.3758/s13428-023-02155-9},
  abstract = {Due to limitations in the resources available for carrying out
  reaction time (RT) experiments, researchers often have to choose between
  testing relatively few participants with relatively many trials each or
  testing relatively many participants with relatively few trials each. To
  compare the experimental power that would be obtained under each of these
  options, I simulated virtual experiments using subsets of participants and
  trials from eight large real RT datasets examining 19 experimental effects.
  The simulations compared designs using the first N T trials from N P randomly
  selected participants, holding constant the total number of trials across all
  participants, N P × N T . The [ N P , N T ] combination maximizing the power
  to detect each effect depended on how the mean and variability of that effect
  changed with practice. For most effects, power was greater in designs having
  many participants with few trials each rather than the reverse, suggesting
  that researchers should usually try to recruit large numbers of participants
  for short experimental sessions. In some cases, power for a fixed total number
  of trials across all participants was maximized by having as few as two trials
  per participant in each condition. Where researchers can make plausible
  predictions about how their effects will change over the course of a session,
  they can use those predictions to increase their experimental power.},
  url = {http://dx.doi.org/10.3758/s13428-023-02155-9},
  file = {miller-2024-how-many-participants-how-many-trials-maximizing-the-power-of-reaction-time-studies.pdf}
}

@ARTICLE{Anikin2025-sw,
  title = {Can I trust this paper?},
  author = {Anikin, Andrey},
  journaltitle = {Psychonomic bulletin \& review},
  publisher = {Springer Science and Business Media LLC},
  pages = {1-15},
  date = {2025-07-16},
  doi = {10.3758/s13423-025-02740-3},
  abstract = {After a decade of data falsification scandals and replication
  failures in psychology and related empirical disciplines, there are urgent
  calls for open science and structural reform in the publishing industry. In
  the meantime, however, researchers need to learn how to recognize tell-tale
  signs of methodological and conceptual shortcomings that make a published
  claim suspect. I review four key problems and propose simple ways to detect
  them. First, the study may be fake; if in doubt, inspect the authors' and
  journal's profiles and request to see the raw data to check for
  inconsistencies. Second, there may be too little data; low precision of effect
  sizes is a clear warning sign of this. Third, the data may not be analyzed
  correctly; excessive flexibility in data analysis can be deduced from signs of
  data dredging and convoluted post hoc theorizing in the text, while violations
  of model assumptions can be detected by examining plots of observed data and
  model predictions. Fourth, the conclusions may not be justified by the data;
  common issues are inappropriate acceptance of the null hypothesis, biased
  meta-analyses, over-generalization over unmodeled variance, hidden confounds,
  and unspecific theoretical predictions. The main takeaways are to verify that
  the methodology is robust and to distinguish between what the actual results
  are and what the authors claim these results mean when citing empirical work.
  Critical evaluation of published evidence is an essential skill to develop as
  it can prevent researchers from pursuing unproductive avenues and ensure
  better trustworthiness of science as a whole.},
  url = {http://dx.doi.org/10.3758/s13423-025-02740-3},
  file = {anikin-2025-can-i-trust-this-paper.pdf}
}

@ARTICLE{Jak2021-yc,
  title = {Meta-analytic structural equation modeling made easy: A tutorial and
  web application for one-stage MASEM},
  author = {Jak, Suzanne and Li, Hongli and Kolbe, Laura and de Jonge, Hannelies
  and Cheung, Mike W-L},
  journaltitle = {Research synthesis methods},
  publisher = {Wiley},
  volume = {12},
  pages = {590-606},
  date = {2021-09},
  doi = {10.1002/jrsm.1498},
  abstract = {Meta-analytic structural equation modeling (MASEM) refers to
  fitting structural equation models (SEMs) (such as path models or factor
  models) to meta-analytic data. Currently, fitting MASEMs may be challenging
  for researchers that are not accustomed to working with R software and
  packages. Therefore, we developed webMASEM; a web application for MASEM. This
  app implements the one-stage MASEM approach, and allows users to apply MASEM
  in a user-friendly way. The aim of this article is to provide a tutorial on
  one-stage MASEM and a practical guide to webMASEM. We will pay specific
  attention to how the data should be structured and prepared for webMASEM,
  because mistakes in this step may lead to faulty results without receiving an
  error message. The use of webMASEM is illustrated with an analysis of a
  meta-analytic path model in which the path coefficients are moderated by a
  study-level variable, a meta-analytic factor model in which the factor
  loadings are moderated by a study-level variable, and a meta-analytic panel
  model in which the effects are moderated by a study-level variable. All used
  datafiles and R scripts are available online.},
  url = {http://dx.doi.org/10.1002/jrsm.1498},
  file = {jak-2021-meta-analytic-structural-equation-modeling-made-easy-a-tutorial-and-web-application-for-one-stage-masem.pdf}
}

@ARTICLE{Sheng2016-ei,
  title = {Analyzing matrices of meta-analytic correlations: current practices
  and recommendations},
  author = {Sheng, Zitong and Kong, Wenmo and Cortina, Jose M and Hou, Shuofei},
  journaltitle = {Research synthesis methods},
  publisher = {Wiley},
  volume = {7},
  pages = {187-208},
  date = {2016-06},
  doi = {10.1002/jrsm.1206},
  abstract = {Researchers have become increasingly interested in conducting
  analyses on meta-analytic correlation matrices. Methodologists have provided
  guidance and recommended practices for the application of this technique. The
  purpose of this article is to review current practices regarding analyzing
  meta-analytic correlation matrices, to identify the gaps between current and
  best practices, and to offer a comprehensive set of recommendations regarding
  the planning, collection, analysis, and interpretation of studies that utilize
  meta-analytic correlation matrices. Copyright © 2016 John Wiley \& Sons, Ltd.},
  url = {http://dx.doi.org/10.1002/jrsm.1206},
  file = {sheng-2016-analyzing-matrices-of-meta-analytic-correlations-current-practices-and-recommendations.pdf}
}
